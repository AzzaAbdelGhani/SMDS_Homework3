---
title: "Homework 3"
author: "Group B: Abdalghani, Demirbilek, Morichetti, Zambon"
date: "Spring 2020"
output:
  html_document:
    toc: no
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/', fig.height = 10, fig.width = 10)
```

```{r setup, include=FALSE}
library(MASS)
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# {.tabset}

## LEC {.tabset}

### Exercise 1


Compute the bootstrap-based confidence interval for the $score$ dataset using the studentized method.


*solution :*



```{r basic 1, echo=TRUE}
#loading data 
score <- read.table("files/student_score.txt", header = TRUE)

# function fo compute the parameter of interest --> eignratio 
psi_fun <- function(data) {eig <- eigen(cor(data))$values
                           return(max(eig) / sum(eig))}
psi_obs <- psi_fun(score)
#psi_obs

# estimate the bootstrap-based standard error, SE_boot:
# and estimate the Standard error from each bootstrap sample using jackknife: 
n <- nrow(score) 
B <- 10^4
s_vect <- rep(0,B)
SE_jack <- rep(0,B)
s_tmp <- rep(0, n)


for(i in 1:B) 
{
  ind <- sample(1:n, n, replace = TRUE)
  s_vect[i] <- psi_fun(score[ind,])
  for(j in 1:n) {s_tmp[j] <- psi_fun(score[ind,][-j,])}
  SE_jack[i] <- sqrt(((n - 1)/n) * sum((s_tmp - mean(s_tmp))^2))
  
}
```

```{r basic 2, echo=TRUE}

SE_boot <- sd(s_vect)
wald_ci <- psi_obs + c(-1, 1) * 1.96 * SE_boot 
print("Wald-type confidence interval :")
wald_ci

hist.scott(s_vect, main = "Wald-type CI")
abline(v = psi_obs, col = 2)
abline(v = wald_ci, col = "blue")


#percentile method : 
perc_ci <- quantile(s_vect, prob=c(0.025, 0.975))
attr(perc_ci, "names") <- NULL
perc_ci
hist.scott(s_vect, main = "Percentile method")
abline(v = psi_obs, col = 2)
abline(v = perc_ci, col = "blue")


#studentized Method
z_vect <- (s_vect - psi_obs)/SE_jack

#studentized bootstrap confidence interval :
stud_ci <- psi_obs - SE_boot * quantile(z_vect, prob=c(0.975, 0.025))
attr(stud_ci, "names") <- NULL 
print("Studentized bootstrap confidence interval :")
stud_ci
hist.scott(s_vect, main = "Studentized method")
abline(v = psi_obs, col = 2)
abline(v = stud_ci, col = "blue")

```

Comparing the Studentized bootstrap confidence interval and the Wald-type interval, and taking the point estimate as a reference, as given in the lecture the percentile confidence interval is wider on the left side and shorter on the right side. However, the studentized bootstrap confidence interval is wider on both sides (left and right).



### Exercise 2

Compute bootstrap-based confidence intervals for the $score$ dataset using the $boot$ package.


*solution :*

```{r basic 3, echo=TRUE}
library(boot)

psi_fun_boot = function(x, indices){
  data <- x[indices,]
  eig <- eigen(cor(data))$values
  return(max(eig) / sum(eig))
}

#bootstrap variances needed for studentized intervals
psi_fun_boot_var = function(x, indices){
  data <- x[indices,]
  b <- boot(score, psi_fun_boot, 100)
  eig <- eigen(cor(data))$values
  return(c(max(eig) / sum(eig), var(b$t)))
}

score <- read.table("files/student_score.txt", header = TRUE)
boot_results <- boot(score, psi_fun_boot, 1000)
boot.ci(boot.out = boot_results, , type = c("perc","basic"))

boot_var_results <- boot(score, psi_fun_boot_var, 1000)
boot.ci(boot.out = boot_var_results, type = "stud")

```



## Laboratory {.tabset} 
### Exercise 1

*Solution :*




### Exercise 2


*Solution :*



### Exercise 3


*Solution :*



### Exercise 5 

In $sim$ in the code above, you find the MCMC output which allows to approximate the posterior distribution of our parameter of interest with $S$ draws of $\theta$. Please, produce an histogram for these random draws $\theta(1),…,\theta(S)$, compute the empirical quantiles, and overlap the true posterior distribution.


*Solution :*

```{r basic = 7, echo=TRUE}
#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 10
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))

# No conjugate Prior
library(rstan)
#launch Stan model
data<- list(N=n, y=y, sigma =sqrt(sigma2), mu = mu, tau = sqrt(tau2))
fit <- stan(file="files/normal.stan",
            data = data, chains = 4, iter=2000)

#extract Stan output
sim <- extract(fit)

# QQPlot
library("car")

quantile(sim$theta)
boxplot(sim$theta, horizontal = TRUE, col = "lightgray", main = "Theta Box Plot")

```

from the numerical value of the quantiles it seems the theta distribution is well distribuited around the mean, infact the quantiles are at the same distance from it. Moreover it is immediately clear by looking on the box plot: 
the various theta values are, more or less, simmetrically spreaded around the mean.

```{r basic = 8, echo=TRUE}
qqnorm(sim$theta, pch = 1, frame = FALSE, main = "Normal Q-Q Plot for theta")
qqline(sim$theta, col = "steelblue", lwd = 2)

```

the Q-Q plot shows us what suppose until know, the theta distribution is distribuited like a normal distribution, even if a there is a tiny variability on the tails. So, we may accept the normality assumption.

```{r basic=9, echo=TRUE}

# plot of the simulated posterior and the true posterior
hist(sim$theta, probability = TRUE, main = "Theta's Histogram", xlab = "theta")
curve(dnorm(x, mu_star, sd_star), 
     xlab=expression(theta), ylab="", col="blue", lwd=2,
     cex.lab=2, add=T)
```

### Exercise 6

Launch the following line of $R$ code:
```{r basic= 10, echo=TRUE}
posterior <- as.array(fit)
```

Use now the $bayesplot$ package. Read the help and produce for this example, using the object posterior, the following plots:

* posterior intervals.
* posterior areas.
* marginal posterior distributions for the parameters.


Quickly comment.

*Solution :*
```{r basic =11 , echo=TRUE}
library(bayesplot)
posterior <- as.array(fit)

# numerical intervals
mcmc_intervals_data(posterior)[1, ]

# numerical areas
mcmc_areas_data(posterior)

# posterior distribution plot
plot_title <- ggtitle("Marginal Posterior distribution for theta", "with median and 80% interval")
mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title
```

The bayes plot library provide us a posterior-plot of the our posterior, 
and it shows us an approximately normal distribution centered in 2.6 for the theta parameter.

```{r basic =12 , echo=TRUE}
# interesting things

# density plot
mcmc_hex(posterior)

# autocorrelation plot
mcmc_acf(posterior)
```

The acf shows us how the parameter is basically no-autocorrelated because
the ac curve is quite close to zero in all chains.

```{r basic =13 , echo=TRUE}

# rhat analysis
rhats <- rhat(fit)
mcmc_rhat(rhats)+ yaxis_text(hjust = 1)
```


The rhat is quite close to 1, that is good for convergence purposes.

```{r basic =14 , echo=TRUE}
# neff analysis
ratios <- neff_ratio(fit)
mcmc_neff(ratios, size = 3) + yaxis_text(hjust = 1)
```


From the plot analysis we see that the neef of the parameter is not so high.

```{r basic =15 , echo=TRUE}
# combo plot
combo_plot <- c("areas", "trace", "hist", "hex")
color_scheme_set("mix-blue-red")
mcmc_combo(
  posterior,
  pars = c("theta", "lp__"),
  combo = combo_plot,
  gg_theme = ggplot2::theme_gray() + legend_none()
)
```

### Exercise 7

Suppose you receive $n=15$ phone calls in a day, and you want to build a model to assess their average length. Your likelihood for each call length is $yi∼Poisson(\lambda)$. Now, you have to choose the prior $π(λ)$. Please, tell which of these priors is adequate to describe the problem, and provide a short motivation for each of them:

1. $π(\lambda)=Beta(4,2);$

2. $π(\lambda)=Normal(1,2);$

3. $π(\lambda)=Gamma(4,2);$

Now, compute your posterior as $\pi(\lambda|y)∝L(\lambda;y)\pi(\lambda)$ for the selected prior. If your first choice was correct, you will be able to compute it analitically.

*solution :*

Remining that a prior distribution is our prior belief on the $\lambda$ paramenter, we may observe the following notes:

1. The Beta distribution is a continuous distribution but his support is defined on the range $[0, 1]$, and means to consider the everage time in days or more that is not realistic.So, it is not convinient to use it because $\lambda$ belongs to the range $[0, +\infty)$. Moreover, the Beta distribution is usually associated with the Bernoulli distribution.

2. The Normal distribution is a continuous distribution and it considers a larger set of values in respect to the Beta distribution but, also in this case, it is not a suitable prior distribution for $\lambda$ because it is defined on the range $(-\infty, +\infty)$, so it allows negative values that are not acceptable for $\lambda$.

3. The Gamma distribution is a continuous distribution defined on the range $[0, +\infty)$, so it is well defined for the $\lambda$ parameter. Moreover, if the likelihood is rapresented by a Poisson distribution (like in this case), the Gamma distribution is not just a prior distribution but it is a conjugate prior. It means the posterior and the prior distributions belong to the same probability distribution family and this is great because we are able to analitically compute the posterior distribution.

As a final thought, we may also consider times and computational costs: the gamma as posterior distribution is a well known distribution while the posteriors given by the others priors force to use the MCMC algorithm and it might be expensive and gives an approximately solution.

We may consider a general case in which we have a Poisson model $X_1, X_2, \dots, X_n$ ~ $Poisson(\lambda)$ where $X_1, X_2, \dots, X_n$ are iid. While as a prior distribution suppose to have a $Gamma(\alpha, \beta)$. Then, the posterior distribution is defined as follows:

$p(\lambda | X) \propto p(X | \lambda) \cdot p(\lambda)$
$\propto \prod_{i = 1}^{n}(\frac{\lambda^{x_i} \cdot e^{-\lambda}}{x_i !}) \cdot \frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \lambda^{\alpha - 1} \cdot e^{\beta \cdot \lambda}$
$\propto \prod_{i = 1}^{n}(\lambda^{x_i}) \cdot e^{-n \cdot \lambda} \cdot \lambda^{\alpha - 1} \cdot e^{\beta \cdot \lambda} = \lambda^{(s + \alpha) - 1} \cdot e^{-(n + \beta) \cdot \lambda}$

Where we have not compute the normalization constant and we ignored constants that do not depend on $\lambda$.
So, the posterior distribution belongs to a gamma distribution $Gamma(s + \alpha, n \cdot \beta)$ in which $s = x_1 + x_2 + \cdots + x_n$; in the our specific case the posterior distribution belongs to a $Gamma(s + 4, 15 \cdot 2)$.



```{r basic= 16 , echo=TRUE}
#input values


#true mean
theta_sample <- 2
#likelihood variance
sigma2 <- 2
#sample size
n <- 15
#prior mean
mu <- 7
#prior variance
tau2 <- 2

#generate some data
set.seed(123)
y <- rnorm(n,theta_sample, sqrt(sigma2))

#posterior mean
mu_star <- ((1/tau2)*mu+(n/sigma2)*mean(y))/( (1/tau2)+(n/sigma2))
#posterior standard deviation
sd_star <- sqrt(1/( (1/tau2)+(n/sigma2)))


curve(dnorm(x, theta_sample, sqrt(sigma2/n)),xlim=c(-4,15), lty=2, lwd=1, col="black", ylim=c(0,1.4), 
      ylab="density", xlab=expression(theta))

curve(dnorm(x, mu, sqrt(tau2) ), xlim=c(-4,15), col="red", lty=1,lwd=2,  add =T)
curve(dnorm(x, mu_star, sd_star), 
      xlab=expression(theta), ylab="", col="blue", lwd=2, add=T)  
legend(8.5, 0.7, c("Prior", "Likelihood", "Posterior"), 
       c("red", "black", "blue", "grey" ), lty=c(1,2,1),lwd=c(1,1,2), cex=1)

```
### Exercise 8


**Go to this link: [rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), and follow the instructions to download and install the `rstan` library. Once you did it succesfully, open the file model called `biparametric.stan`, and replace the line:**

`target+ = cauchy_lpdf(sigma | 0, 2.5);`

**with the following one:**

`target+ = uniform_lpdf(sigma | 0.1, 10);`

**Which prior are you now assuming for your parameter**$\sigma$**? Reproduce the same plots as above and briefly comment. **

**Solution:**

We are assuming uniform distribution for parameter $\sigma$ such that $\sigma \sim \text{Unif}(0.1,10)$.

```{r basic= 17,echo=TRUE}
library("bayesplot")
library("rstan")
library("ggplot2")

theta_sample <- 2
sigma2 <- 2
n <- 10
y <- rnorm(n,theta_sample, sqrt(sigma2))

data <- list(N=n, y=y, a=-10, b=10)
fit <- stan(file ="files/biparametric.stan", data = data , chains = 4, iter=2000, refresh=-1)
sim <- extract(fit)
posterior_biv <- as.matrix(fit)

theta_est <- mean(sim$theta)
theta_var <- var(sim$theta)
sigma_est <- mean(sim$sigma)
sigma_var <- var(sim$sigma)
c(theta_est,theta_var, sigma_est,sigma_var)
traceplot(fit, pars=c("theta", "sigma"))

plot_title <- ggtitle("Posterior distributions",
                      "with medians and 80% intervals")

mcmc_areas(posterior_biv, 
           pars = c("theta","sigma"), 
           prob = 0.8) + plot_title

```

From the traceplot, we can easily say that chains are converged meaning they run around some values of the parameter space. There is no significant difference between old and new posterior distribution plots so we may say that prior distributions don't provide much information about posterior distributions.






### Exercise 9

**Reproduce the first plot above for the soccer goals, but this time by replacing Prior 1 with a **$\text{Gamma}(2,4).$**Then, compute the final Bayes factor matrix `(BF_matrix)` with this new prior and the other ones unchanged, and comment. Is still Prior 2 favorable over all the others? **

**Solution:**

Likelihood and prior distributions are given in labratory file. Prior one changed from $\text{Gamma}(4.57,1.43)$ to $\text{Gamma}(2,4)$ (average $\alpha/\beta = 0.5$) and first plot reproduced below. 

```{r basic= 18,echo=TRUE,  message=FALSE, warning=FALSE}
library(LearnBayes)
data(soccergoals)

y <- soccergoals$goals

#write the likelihood function via the gamma distribution


lik_pois<- function(data, theta){
  n <- length(data)
  lambda <- exp(theta)
  dgamma(lambda, shape =sum(data)+1, scale=1/n)
}

 prior_gamma <- function(par, theta){
  lambda <- exp(theta)
  dgamma(lambda, par[1], rate=par[2])*lambda  
}

 prior_norm <- function(npar, theta){
 lambda=exp(theta)  
 (dnorm(theta, npar[1], npar[2]))
  
}

lik_pois_v <- Vectorize(lik_pois, "theta")
prior_gamma_v <- Vectorize(prior_gamma, "theta")
prior_norm_v <- Vectorize(prior_norm, "theta")


#likelihood
 curve(lik_pois_v(theta=x, data=y), xlim=c(-1,4), xlab=expression(theta), ylab = "density", lwd =2 )
#prior 1
 curve(prior_gamma_v(theta=x, par=c(2, 4)), lty =2, col="red", add = TRUE, lwd =2)
#prior 2 
 curve(prior_norm_v(theta=x, npar=c(1, .5)), lty =3, col="blue", add =TRUE, lwd=2)
#prior 3 
 curve(prior_norm_v(theta=x, npar=c(2, .5)), lty =4, col="green", add =TRUE, lwd =2)
#prior 4 
  curve(prior_norm_v(theta=x, npar=c(1, 2)), lty =5, col="violet", add =TRUE, lwd =2)
  legend(2.6, 1.8, c("Lik.", "Ga(2,4)", "N(1, 0.25)", "N(2,0.25)","N(1, 4)" ),
  lty=c(1,2,3,4,5), col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)


```
In previous plot, `Prior 1` and `Prior 2` coincided but now they are different because of the change we made. Expected value of likelihood centered around $0.5$ and each prior offered different estimation and variance. `Prior 3` is concentrated around 2 which seems higher than other priors. `Prior 4` shows more spreaded distribution so it is the less informative one among the others. 

```{r basic= 19,echo=TRUE,  message=FALSE, warning=FALSE}
logpoissongamma <- function(theta, datapar){
   data <- datapar$data
   par <- datapar$par
   lambda <- exp(theta)
   log_lik <- log(lik_pois(data, theta))
   log_prior <- log(prior_gamma(par, theta))
   return(log_lik+log_prior)
}

logpoissongamma.v <- Vectorize( logpoissongamma, "theta")


logpoissonnormal <- function( theta, datapar){
 data <- datapar$data
 npar <- datapar$par
 lambda <- exp(theta)
 log_lik <- log(lik_pois(data, theta))
 log_prior <- log(prior_norm(npar, theta))
  return(log_lik+log_prior)
}  
logpoissonnormal.v <- Vectorize( logpoissonnormal, "theta")

#log-likelihood
curve(log(lik_pois(y, theta=x)), xlim=c(-1,4),ylim=c(-20,2), lty =1,
   ylab="log-posteriors", xlab=expression(theta))
#log posterior 1
curve(logpoissongamma.v(theta=x, list(data=y, par=c(2, 4))), col="red", xlim=c(-1,4),ylim=c(-20,2), lty =1, add =TRUE)
#log posterior 2
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(1, .5))), lty =1, col="blue",  add =TRUE)
#log posterior 3
 curve(logpoissonnormal.v( theta=x, datapar <- list(data=y, par=c(2, .5))), lty =1, col="green", add =TRUE, lwd =2)
#log posterior 4
  curve(logpoissonnormal.v( theta=x, list(data=y, par=c(1, 2))), lty =1, col="violet", add =TRUE, lwd =2)
 legend(2.6, 1.3, c( "loglik", "lpost 1", "lpost 2", "lpost 3", "lpost 4" ),
  lty=1, col=c("black", "red", "blue", "green", "violet"),lwd=2, cex=0.9)
```
Curves of posterior distributions are quite similar among the each other. They are close to he log-likeligood function. The only curve stands out is the one more in constrast with likelihood. Comparasions of these models will be made by using bayes factor.

```{r basic= 20, echo=TRUE,  message=FALSE, warning=FALSE}
datapar <- list(data=y, par=c(2, 4))
fit1 <- laplace(logpoissongamma, .5, datapar)
datapar <- list(data=y, par=c(1, .5))
fit2 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(2, .5))
fit3 <- laplace(logpoissonnormal, .5, datapar)
datapar <- list(data=y, par=c(1, 2))
fit4 <- laplace(logpoissonnormal, .5, datapar)

postmode <- c(fit1$mode, fit2$mode, fit3$mode, fit4$mode )
postsds <- sqrt(c(fit1$var, fit2$var, fit3$var, fit4$var))
logmarg <- c(fit1$int, fit2$int, fit3$int, fit4$int)
cbind(postmode, postsds, logmarg)
```
Here we found the posterior mode, log marginal likelihood and posterior standard deviations. Posterior mode of `Prior 1` is bit smaller than others. It is due to influence of the prior distribution concentrated around smaller values compared the likelihood.

```{r basic= 21, echo=TRUE,  message=FALSE, warning=FALSE}
BF_matrix <- matrix(1, 4,4)
for (i in 1:3){
  for (j in 2:4){
   BF_matrix[i,j]<- exp(logmarg[i]-logmarg[j])
   BF_matrix[j,i]=(1/BF_matrix[i,j]) 
  }
}

round_bf <- round(BF_matrix,3)
round_bf
```
`Prior 2` is still favored over other priors. The change that we did to `Prior 1` made negative impact and it perform worse than previous prior choice and still every prior is favored over `Prior 3`.




### Exercise 10

**Let $y=(1,0,0,1,0,0,0,0,0,1,0,0,1,0)$ collect the results of tossing $n=14$ times an unfair coin, where 1 denotes _heads_ and 0 _tails_, and $p=\text{Prob}(y_i=1)$.**

* **Looking at the `Stan` code for the other models, write a short Stan Beta-Binomial model, where $p$ has a $\text{Beta}(a,b)$ prior with $a=3, b=3$.**
```
The stan file used is:
data{
  int N;
  int y;
  real<lower=0> alpha;
  real<lower=0> beta;
}

parameters{
  real theta;
}

model{
  target+=binomial_lpmf(y|N, theta); 
  target+=beta_lpdf(theta|alpha, beta);
}
```

* **extract the posterior distribution with the function `extract()`**

* **produce some plots with the `bayesplot` package and comment.**

```{r basic= 22, echo=TRUE,  message=FALSE, warning=FALSE}
library(LearnBayes)

y=c(1,0,0,1,0,0,0,0,0,1,0,0,1,0)
n <-  length(y)
heads <- sum(y == 1)
tails <- n - heads

alpha = 3
beta = 3

data<- list(N=n, y=heads, alpha=alpha, beta=beta)
fit <- stan(file="files/beta-binomial.stan", data = data, chains = 4, iter=2000,refresh=-1)
#extract the Stan output
sim <- extract(fit)

summary(sim$theta)
var(sim$theta)

posterior <- as.matrix(fit)

#traceplot
traceplot(fit, pars ="theta")

plot_title <- ggtitle("Posterior distributions","with medians and 80% intervals")

mcmc_areas(posterior, pars = c("theta"), prob = 0.8) + plot_title

```

It can be observed that chains are converged, in other words they run around some values. Estimation is $0.35$ with variance $0.011$ which isn't very large. Second plot shows estimation median which is around $0.35$ with dark blue line and blue shaded area is the 80% confidence interval of median.

* **compute analitically the posterior distribution and compare it with the `Stan` distribution.**

Likelihood is binomial distribution with the following parameters: (H denotes heads, T denotes tails)
$$
\text{Bin}(H|\theta,H+T)
$$









